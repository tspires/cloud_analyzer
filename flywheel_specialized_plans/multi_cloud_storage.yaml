name: "Multi-Cloud Storage Optimization"
description: "Comprehensive storage optimization across S3, Azure Blob, and GCS"
variables:
  analyze_lifecycle: "true"
  analyze_access_patterns: "true"
  days_to_analyze: "90"
  
steps:
  - prompt: |
      Analyze object storage across all cloud providers:
      
      AWS S3:
      1. List all buckets: aws s3api list-buckets
      2. For each bucket:
         - Get size: aws s3 ls s3://BUCKET --recursive --summarize | grep "Total Size"
         - Get storage class distribution: aws s3api list-objects-v2 --bucket BUCKET --query 'Contents[].StorageClass' | sort | uniq -c
         - Get lifecycle policies: aws s3api get-bucket-lifecycle-configuration --bucket BUCKET
         - Get access logs: aws s3api get-bucket-logging --bucket BUCKET
      
      Azure Blob:
      1. List storage accounts: az storage account list
      2. For each account:
         - List containers: az storage container list --account-name ACCOUNT
         - Get metrics: az monitor metrics list --resource RESOURCE_ID --metric "UsedCapacity" "BlobCount"
         - Get lifecycle policies: az storage account management-policy show --account-name ACCOUNT
      
      GCS:
      1. List buckets: gsutil ls
      2. For each bucket:
         - Get size: gsutil du -s gs://BUCKET
         - Get storage classes: gsutil ls -L -b gs://BUCKET
         - Get lifecycle: gsutil lifecycle get gs://BUCKET
    description: "Discover storage resources"
    saveOutput: "storage_inventory"

  - prompt: |
      Analyze access patterns for storage inventory: {{storage_inventory}}
      
      For each bucket/container:
      1. Access frequency analysis:
         - Objects not accessed in 30/60/90 days
         - Access patterns by prefix/folder
         - Request metrics (GET/PUT/DELETE)
      
      2. Size distribution:
         - Count of objects by size buckets (< 1MB, 1-10MB, 10-100MB, > 100MB)
         - Identify many small files that could be consolidated
         - Find large files that could use multipart upload
      
      3. Analyze existing lifecycle policies effectiveness
      
      Calculate current monthly costs by storage class.
    description: "Analyze access patterns"
    saveOutput: "access_analysis"

  - prompt: |
      Based on storage inventory and access patterns:
      - Inventory: {{storage_inventory}}
      - Access Analysis: {{access_analysis}}
      
      Generate optimization recommendations:
      
      1. Lifecycle Policy Optimization:
         - Move infrequently accessed data to cooler storage tiers
         - Set up intelligent tiering where available
         - Configure expiration for temporary data
         - Specific rules by prefix/tag
      
      2. Storage Class Migration:
         - Calculate savings from moving data to appropriate storage classes
         - Standard → Standard-IA/Cool (accessed < 30 days)
         - Standard-IA → Glacier/Archive (accessed < 90 days)
         - Consider Intelligent Tiering for unpredictable access
      
      3. Data Management:
         - Delete orphaned/unused data
         - Compress large text/log files
         - Consolidate small files into archives
         - Remove old versions beyond retention needs
      
      4. Cross-Region Optimization:
         - Identify duplicate data across regions
         - Optimize replication settings
         - Use edge locations for frequently accessed content
      
      Provide specific savings calculations for each recommendation.
    description: "Generate storage optimizations"
    saveOutput: "storage_optimizations"

  - prompt: |
      Create implementation scripts for: {{storage_optimizations}}
      
      Generate safe, incremental scripts that:
      
      1. Lifecycle Policy Updates:
         - Backup existing policies
         - Create new optimized policies
         - Test on small subset first
         - Roll out incrementally
      
      2. Storage Class Transitions:
         - List objects to transition
         - Calculate transition costs
         - Create batch jobs for transitions
         - Verify successful transitions
      
      3. Cleanup Scripts:
         - Identify deletion candidates
         - Create deletion lists for review
         - Implement with dry-run first
         - Log all deletions
      
      Include cost tracking commands to measure savings.
    description: "Generate implementation scripts"
    saveOutput: "storage_scripts"

  - prompt: |
      Design a storage cost monitoring dashboard based on the optimizations: {{storage_optimizations}}
      
      Create queries and alerts for:
      1. Storage growth rate by class
      2. Lifecycle policy effectiveness
      3. Access pattern changes
      4. Cost trends by bucket/container
      5. Data transfer costs
      
      Provide CloudWatch/Azure Monitor/GCP Monitoring configurations.
    description: "Create monitoring plan"